import os
import json
import random
import shutil as sh
import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
import PIL


def train_agent(env, agent, exp_params, exp_dirs, enable_eps_recording):

    random.seed(exp_params['seed'])
    np.random.seed(exp_params['seed'])

    train_dir = exp_dirs['training']

    training_eps_count = exp_params['training']['episode_count']
    validate_agent_every_n_eps = exp_params['validation']['validate_agent_every_n_eps']
    validation_eps_count = exp_params['validation']['episode_count']

    train_metrics = {
        'total_eps_count': training_eps_count,
        'validation_step': validate_agent_every_n_eps,
        'win_share': []
    }

    for eps_num in range(1, training_eps_count + 1):
        state, info = env.reset()
        done = False

        while not done:
            action = agent.choose_action(state, mode='exploration')
            next_state, reward, terminated, truncated, info = env.step(action)
            agent.learn(state, action, next_state, reward)
            done = terminated or truncated
            state = next_state

        agent.decrease_exploration_level()

        if eps_num % validate_agent_every_n_eps == 0:
            eps_dir = os.path.join(train_dir, f'eps-{eps_num}')
            os.mkdir(eps_dir)

            val_metrics = validate_agent(env, agent, exp_params, eps_dir, enable_eps_recording)
            agent.save_agent_state(eps_dir)

            print(f"EPISODE # {eps_num} / {training_eps_count}")
            print(f"Win share ({validation_eps_count} eps): {val_metrics['win_share']}")
            print(f"Avg step count ({validation_eps_count} eps): {val_metrics['avg_step_count']}")
            print(f"Agent exploration level (epsilon): {val_metrics['exploration_level']}")
            print('-' * 50)


def validate_agent(env, agent, exp_params, train_eps_dir, enable_eps_recording=False):
    validation_eps_count = exp_params['validation']['episode_count']
    won_eps_count = 0
    eps_step_counts = list()

    for eps_num in range(1, validation_eps_count + 1):
        eps_final_state, eps_step_count, eps_frames = run_episode(env, agent)
        if eps_final_state == env.observation_space.n - 1:
            won_eps_count += 1
        eps_step_counts.append(eps_step_count)
        if enable_eps_recording:
            record_episode(eps_frames, train_eps_dir, exp_params['algorithm_name'], exp_params['exp_id'], eps_num)

    val_metrics = {
        'win_share': round(won_eps_count / validation_eps_count, 3),
        'avg_step_count': round(sum(eps_step_counts) / validation_eps_count, 3),
        'exploration_level': round(agent.exploration_level, 3)
    }

    save_metrics(val_metrics, 'validation', train_eps_dir)
    return val_metrics


def run_episode(env, agent):
    state, info = env.reset()
    step_count = 0
    frames = list()
    done = False

    while not done:
        action = agent.choose_action(state, mode='exploitation')
        next_state, reward, terminated, truncated, info = env.step(action)
        frames.append(env.render())
        done = terminated or truncated
        state = next_state
        step_count += 1

    frames.extend([env.render()] * 3)
    return state, step_count, frames


def evaluate_agent(env, agent, exp_params, exp_dirs):
    for eps_num in range(1, exp_params['evaluation']['episode_count'] + 1):
        eps_final_state, eps_step_count, eps_frames = run_episode(env, agent)
        record_episode(eps_frames, exp_dirs['evaluation'], exp_params['algorithm_name'], exp_params['exp_id'], eps_num)

        print(f'EPISODE # {eps_num}')
        if eps_final_state == env.observation_space.n - 1:
            print('Agent reached the Goal üèÜ')
        else:
            print('Agent was defeated ‚ò†Ô∏è')
        print(f'Number of steps: {eps_step_count}')
        print('-' * 50)


def record_episode(eps_frames, records_dir, agent_name, exp_id, eps_num):
    record_path = os.path.join(records_dir, f'{agent_name}_{exp_id}_eps-{eps_num}.mp4')
    eps_frame_dir = 'episode_frames'
    os.mkdir(eps_frame_dir)

    for i, frame in enumerate(eps_frames):
        PIL.Image.fromarray(frame).save(os.path.join(eps_frame_dir, f'frame-{i+1}.png'))

    os.system(f'ffmpeg -r 2 -i {eps_frame_dir}/frame-%1d.png -vcodec libx264 -b 10M -y "{record_path}"');
    sh.rmtree(eps_frame_dir)


def create_exp_dirs(scenarios_dir, exp_params):
    dirs = dict()
    dirs['exp'] = os.path.join(scenarios_dir, exp_params["scenario_name"], exp_params["algorithm_name"], exp_params["exp_id"])
    dirs['training'] = os.path.join(dirs['exp'], 'training')
    dirs['evaluation'] = os.path.join(dirs['exp'], 'evaluation')
    os.makedirs(dirs['training'], exist_ok=True)
    os.makedirs(dirs['evaluation'], exist_ok=True)
    return dirs


def save_exp_params(params, exp_dir):
    params_path = os.path.join(exp_dir, 'experiment_params.json')
    with open(params_path, 'w') as f:
        json.dump(params, f)


def save_metrics(metrics, metrics_type, train_eps_dir):
    metrics_path = os.path.join(train_eps_dir, f'{metrics_type}_metrics.json')
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f)


def load_best_agent_for_train_step(train_eps_num, training_dir, agent):

    eps_dir = os.path.join(training_dir, f'eps-{train_eps_num}')
    val_metrics_path = os.path.join(eps_dir, 'validation_metrics.json')
    with open(val_metrics_path, 'r') as f:
        val_metrics = json.load(f)
    
    print('Win share: ', val_metrics['win_share'])
    print('Avg step count: ', val_metrics['avg_step_count'])

    agent_state_dir = os.path.join(training_dir, f'eps-{train_eps_num}')
    agent.load_agent_state(agent_state_dir)


def create_environment(map_size, stochastic_env):
    return gym.make('FrozenLake-v1', map_name=f'{map_size}x{map_size}', is_slippery=stochastic_env, render_mode='rgb_array')


def get_validation_metrics(scenarios_dir, scenario_name, exp_id):

    exp_dir = os.path.join(scenarios_dir, scenario_name, exp_id)
    exp_params_path = os.path.join(exp_dir, 'experiment_params.json')
    training_dir = os.path.join(exp_dir, 'training')

    with open(exp_params_path, 'r') as f:
        exp_params = json.load(f)

    exp_win_shares = list()
             
    for ep in os.listdir(training_dir):
        metrics_path = os.path.join(training_dir, ep, 'validation_metrics.json')
        with open(metrics_path, 'r') as f:
            metrics = json.load(f)
            exp_win_shares.append(metrics['win_share'])

    return exp_win_shares


def display_validation_plot(exp_win_shares, validation_step=100, total_steps=10000, model_name='Agent'):
    
    fig = plt.figure(figsize=(10, 6))

    iter_means = np.mean(exp_win_shares, axis=0).round(3)
    iter_stds = np.std(exp_win_shares, axis=0).round(3)
    std_bounds = np.array([[avg - std, avg + std] for avg, std in list(zip(iter_means, iter_stds))])
    validation_steps = range(0, total_steps, validation_step)

    plt.plot(validation_steps, iter_means)
    plt.fill_between(validation_steps, std_bounds[:,0], std_bounds[:,1], alpha=.3)

    plt.ylim(0, 1)
    plt.xlabel('Training episodes')
    plt.ylabel('Win share')
    plt.title(f'Assessment of {model_name} learning process')
    plt.grid()
